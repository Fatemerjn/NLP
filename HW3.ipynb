{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hazm\n",
      "  Using cached hazm-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n",
      "  Using cached fasttext_wheel-0.9.2-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Collecting flashtext<3.0,>=2.7 (from hazm)\n",
      "  Using cached flashtext-2.7-py2.py3-none-any.whl\n",
      "Collecting gensim<5.0.0,>=4.3.1 (from hazm)\n",
      "  Using cached gensim-4.3.2.tar.gz (23.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\fateme\\documents\\github\\nlp\\myenv\\lib\\site-packages (from hazm) (3.8.1)\n",
      "Collecting numpy==1.24.3 (from hazm)\n",
      "  Using cached numpy-1.24.3.tar.gz (10.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [33 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\Fateme\\Documents\\GitHub\\NLP\\myenv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "      main()\n",
      "    File \"C:\\Users\\Fateme\\Documents\\GitHub\\NLP\\myenv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Fateme\\Documents\\GitHub\\NLP\\myenv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n",
      "      backend = _build_backend()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Fateme\\Documents\\GitHub\\NLP\\myenv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n",
      "      obj = import_module(mod_path)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Fateme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "      return _bootstrap._gcd_import(name[level:], package, level)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "    File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "    File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "    File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "    File \"C:\\Users\\Fateme\\AppData\\Local\\Temp\\pip-build-env-lx2_w9um\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
      "      import setuptools.version\n",
      "    File \"C:\\Users\\Fateme\\AppData\\Local\\Temp\\pip-build-env-lx2_w9um\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
      "      import pkg_resources\n",
      "    File \"C:\\Users\\Fateme\\AppData\\Local\\Temp\\pip-build-env-lx2_w9um\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
      "      register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hazm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhazm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Normalizer, word_tokenize, stopwords_list, Stemmer\n\u001b[0;32m     13\u001b[0m file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(file_path)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hazm'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score\n",
    "from hazm import Normalizer, word_tokenize, stopwords_list, Stemmer\n",
    "\n",
    "file_path='dataset.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Content_1', 'Genre']].dropna()\n",
    "\n",
    "normalizer = Normalizer()\n",
    "stop_words = set(stopwords_list())\n",
    "stemmer = Stemmer()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = normalizer.normalize(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Content_1'] = df['Content_1'].astype(str).fillna('')\n",
    "\n",
    "# Normalize the content using Hazm\n",
    "df['Content_1'] = df['Content_1'].apply(normalize_text)\n",
    "\n",
    "genre_mapping = {\n",
    "    'Drama': ['Drama', 'Human Interest & Society', 'History', 'Mystery'],\n",
    "    'Comedy': ['Comedy', 'Arts & Literature', 'Romance', 'Portrait'],\n",
    "    'Action': ['Action', 'Culture & Traditions', 'Architecture & Urbanism', 'Music'],\n",
    "    'Crime': ['Crime', 'Family', 'Experimental', 'Nature & Wildlife'],\n",
    "    'Adventure': ['Adventure', 'War', 'Horror', 'Thriller', 'Animation']\n",
    "}\n",
    "\n",
    "# Function to map genres to main genres\n",
    "def map_genre(genre):\n",
    "    for main_genre, similar_genres in genre_mapping.items():\n",
    "        if genre in similar_genres:\n",
    "            return main_genre\n",
    "    return None\n",
    "\n",
    "# Apply the mapping\n",
    "df['Main_Genre'] = df['Genre'].apply(map_genre)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['Genre_encoded'] = label_encoder.fit_transform(df['Main_Genre'])\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Genre_encoded'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['Genre_encoded'])\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(train_df[['Content_1']], train_df['Genre_encoded'])\n",
    "train_df_resampled = pd.DataFrame({'Content_1': X_train_resampled['Content_1'], 'Genre_encoded': y_train_resampled})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n",
    "\n",
    "\n",
    "class PersianMovieGenreDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts.reset_index(drop=True)  # Ensure proper indexing\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = PersianMovieGenreDataset(train_df_resampled['Content_1'], train_df_resampled['Genre_encoded'], tokenizer)\n",
    "val_dataset = PersianMovieGenreDataset(val_df['Content_1'], val_df['Genre_encoded'], tokenizer)\n",
    "test_dataset = PersianMovieGenreDataset(test_df['Content_1'], test_df['Genre_encoded'], tokenizer)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('HooshvareLab/bert-fa-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda p: {'accuracy': accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))}\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)\n",
    "\n",
    "model.save_pretrained('./parsbert_movie_genre_classifier')\n",
    "tokenizer.save_pretrained('./parsbert_movie_genre_classifier')\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_true = test_df['Genre_encoded'].to_numpy()\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score (Macro): {f1_macro}')\n",
    "print(f'F1 Score (Micro): {f1_micro}')\n",
    "print(f'Precision (Macro): {precision}')\n",
    "print(f'Recall (Macro): {recall}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=label_encoder.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "def predict_genre(summary, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(summary, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs['input_ids'], attention_mask=inputs['attention_mask']).logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    return label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "# Example usage\n",
    "\n",
    "summary = \"سالار از طریق چوب بری قاچاق امرار معاش می کند. او یک بار هنگام قطع درخت، دستگیر، اره موتوری اش ضبط و روانه زندان می شود تا دوران محکومیت یک ساله اش را سپری کند. باجی همسر، جلال پسر، ماجان دختر - اعضای خانواده سالار - در غیاب او باید هزینه اره را به صاحب آن میرزا آقا بپردازند. ضمن اینکه ، قبل از واقعه اخیر قرار بوده تا ماجان با لطیف، سرباز اهری پاسگاه روستا ازدواج کند اما بعد از دستگیری سالار، چون باجی، لطیف را هنگام دستگیری شوهرش دیده است، پس در مورد ازدواج لطیف و ماجان تغییر عقیده می دهد. اما لطیف با پیگیری مستمر سعی می کند تا باجی را متقاعد به بی گناهی خود در دستگیری سالار و ازدواج خود با ماجان قانعش کند. جلال که رفتار نامناسب میرزا آقا را با مادرش به خاطر اره ضبط شده، می بیند، با او که رئیس قاچاقچیان چوب جنگل است توافق می کند تا درازای جبران خسارت وی، هر روزه برای فرج و عطا - قاچاقچیان چوب - اره را به میان جنگل ببرد. تحرکات پنهانی و غیبت های مستمر جلال، موجب حساسیت و نگرانی خانواده اش می شود. وقتی ماجان این نگرانی را با لطیف در میان می گذارد، او به تعقیب جلال در یک صبح زود می پردازد، اما لو میرود و توسط قاچاقچیان گرفتار و مورد ضرب و شتم قرار گرفته و جانش را ازدست میدهد. جلال، که در جریان این حادثه به بی گناهی لطیف و خیانت فرج آگاه شده می گریزد. فرج که دریافته او شاهد واقعه بوده، به تعقیبش می پردازد اما او را نمی یابد. باجی و ماجان که از ماوقع آگاه شده اند در جستجوری جلال برمی آیند و او را در درمانگاه روستا با چهره ای سخت پریشان و در هم می یابند، این زمانی است که سالار از زندان آزاد شده و به روستا بازگشته است. او به محض ورود به روستا سراغ لطیف را می گیرد.\"\n",
    "predicted_genre = predict_genre(summary, model, tokenizer, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Predicted genre: {predicted_genre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
