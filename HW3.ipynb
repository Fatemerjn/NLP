{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Head:\n",
      "                                                 Link            EN_title  \\\n",
      "0  https://www.imvbox.com/watch-persian-movie-ira...   Local Anaesthetic   \n",
      "1  https://www.imvbox.com/watch-persian-movie-ira...         Disturbance   \n",
      "2  https://www.imvbox.com/watch-persian-movie-ira...           Highlight   \n",
      "3  https://www.imvbox.com/watch-persian-movie-ira...               Gilda   \n",
      "4  https://www.imvbox.com/watch-persian-movie-ira...  Atmosphere Station   \n",
      "\n",
      "     PENGLISH_title   PERSIAN_title  \\\n",
      "0  Bi Hessie Mozeie    بی‌حسی موضعی   \n",
      "1         Ashoftegi        آشفته گی   \n",
      "2           Haylayt         هایلایت   \n",
      "3            Geelda           گیلدا   \n",
      "4  Istgahe Atmosfer  ایستگاه اتمسفر   \n",
      "\n",
      "                                           Content_1  \\\n",
      "0  جلال‌، دانشجوی سابق رشته فلسفه، متوجه می‌شود خ...   \n",
      "1  «آشفته‌گی» رئالیستی و اجتماعی نیست. یک فیلم اس...   \n",
      "2  یک تصادف اتومبیل آدم‌هایی را در تقابل با هم قر...   \n",
      "3  گیلدا ماجرای زنی به نام «گیلدا» را روایت می کن...   \n",
      "4  این فیلم روایت گر داستان زندگی زوج جوانی به اس...   \n",
      "\n",
      "                                           Content_2  Score  Year  Genre Time  \n",
      "0  Jalal, a dropouts philosophy student, realizes...    4.8  2018  Drama   73  \n",
      "1  After the murder of his rich twin brother, Bar...    3.8  2018  Crime   78  \n",
      "2  A man and a woman are have a car accident and ...    4.4  2017  Drama   77  \n",
      "3  Gilda who owns a restaurant has a terrible nig...    3.8  2018  Drama   79  \n",
      "4  Vahid and Marjan are a young couple who have g...    5.6  2017  Drama   85  \n",
      "Columns:\n",
      " Index(['Link', 'EN_title', 'PENGLISH_title', 'PERSIAN_title', 'Content_1',\n",
      "       'Content_2', 'Score', 'Year', 'Genre', 'Time'],\n",
      "      dtype='object')\n",
      "Missing Values:\n",
      " Link                0\n",
      "EN_title            0\n",
      "PENGLISH_title      5\n",
      "PERSIAN_title       1\n",
      "Content_1         361\n",
      "Content_2         206\n",
      "Score               0\n",
      "Year                0\n",
      "Genre               0\n",
      "Time                8\n",
      "dtype: int64\n",
      "Cleaned DataFrame Head:\n",
      "                                                 Link            EN_title  \\\n",
      "0  https://www.imvbox.com/watch-persian-movie-ira...   Local Anaesthetic   \n",
      "1  https://www.imvbox.com/watch-persian-movie-ira...         Disturbance   \n",
      "2  https://www.imvbox.com/watch-persian-movie-ira...           Highlight   \n",
      "3  https://www.imvbox.com/watch-persian-movie-ira...               Gilda   \n",
      "4  https://www.imvbox.com/watch-persian-movie-ira...  Atmosphere Station   \n",
      "\n",
      "     PENGLISH_title   PERSIAN_title  \\\n",
      "0  Bi Hessie Mozeie    بی‌حسی موضعی   \n",
      "1         Ashoftegi        آشفته گی   \n",
      "2           Haylayt         هایلایت   \n",
      "3            Geelda           گیلدا   \n",
      "4  Istgahe Atmosfer  ایستگاه اتمسفر   \n",
      "\n",
      "                                           Content_1  \\\n",
      "0  جلال‌، دانشجوی سابق رشته فلسفه، متوجه می‌شود خ...   \n",
      "1  «آشفته‌گی» رئالیستی و اجتماعی نیست. یک فیلم اس...   \n",
      "2  یک تصادف اتومبیل آدم‌هایی را در تقابل با هم قر...   \n",
      "3  گیلدا ماجرای زنی به نام «گیلدا» را روایت می کن...   \n",
      "4  این فیلم روایت گر داستان زندگی زوج جوانی به اس...   \n",
      "\n",
      "                                           Content_2  Score  Year  Genre Time  \n",
      "0  Jalal, a dropouts philosophy student, realizes...    4.8  2018  Drama   73  \n",
      "1  After the murder of his rich twin brother, Bar...    3.8  2018  Crime   78  \n",
      "2  A man and a woman are have a car accident and ...    4.4  2017  Drama   77  \n",
      "3  Gilda who owns a restaurant has a terrible nig...    3.8  2018  Drama   79  \n",
      "4  Vahid and Marjan are a young couple who have g...    5.6  2017  Drama   85  \n",
      "Encoded Labels:\n",
      " 0    7\n",
      "1    5\n",
      "2    7\n",
      "3    7\n",
      "4    7\n",
      "Name: Genre_encoded, dtype: int32\n",
      "Training Set Size: 860\n",
      "Validation Set Size: 108\n",
      "Test Set Size: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fateme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Fateme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688971a48c6141169cd40862a46bb094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0678, 'grad_norm': 17.206127166748047, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.09}\n",
      "{'loss': 3.0929, 'grad_norm': 16.972537994384766, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.19}\n",
      "{'loss': 3.033, 'grad_norm': 15.99864673614502, 'learning_rate': 3e-06, 'epoch': 0.28}\n",
      "{'loss': 2.8632, 'grad_norm': 16.496543884277344, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.37}\n",
      "{'loss': 2.8659, 'grad_norm': 20.564659118652344, 'learning_rate': 5e-06, 'epoch': 0.46}\n",
      "{'loss': 2.7995, 'grad_norm': 18.60843276977539, 'learning_rate': 6e-06, 'epoch': 0.56}\n",
      "{'loss': 2.8122, 'grad_norm': 28.743831634521484, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.65}\n",
      "{'loss': 2.8848, 'grad_norm': 18.807851791381836, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.74}\n",
      "{'loss': 2.7629, 'grad_norm': 30.592498779296875, 'learning_rate': 9e-06, 'epoch': 0.83}\n",
      "{'loss': 2.626, 'grad_norm': 20.519529342651367, 'learning_rate': 1e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bd9e87114f44d9929fabbb3fd806e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.678762912750244, 'eval_runtime': 81.0766, 'eval_samples_per_second': 1.332, 'eval_steps_per_second': 0.173, 'epoch': 1.0}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The `metric_for_best_model` training argument is set to 'eval_f1', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Consider changing the `metric_for_best_model` via the TrainingArguments.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Fateme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2889\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2888\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2889\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_to_check\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eval_f1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 133\u001b[0m\n\u001b[0;32m    123\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[0;32m    124\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    125\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Add your custom metrics computation here\u001b[39;00m\n\u001b[0;32m    130\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m    136\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n",
      "File \u001b[1;32mc:\\Users\\Fateme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Fateme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2365\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2369\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fateme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2796\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2793\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[0;32m   2795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2796\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\Fateme\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2891\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2889\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m metrics[metric_to_check]\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m-> 2891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m   2892\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `metric_for_best_model` training argument is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, which is not found in the evaluation metrics. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2893\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe available evaluation metrics are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider changing the `metric_for_best_model` via the TrainingArguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2894\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m   2896\u001b[0m operator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgreater \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgreater_is_better \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mless\n\u001b[0;32m   2897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2899\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2900\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m operator(metric_value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_metric)\n\u001b[0;32m   2901\u001b[0m ):\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The `metric_for_best_model` training argument is set to 'eval_f1', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Consider changing the `metric_for_best_model` via the TrainingArguments.\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = 'dataset.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"DataFrame Head:\\n\", df.head())\n",
    "print(\"Columns:\\n\", df.columns)\n",
    "print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Ensure 'Content_1' and 'Genre' columns are present\n",
    "if 'Content_1' not in df.columns or 'Genre' not in df.columns:\n",
    "    raise ValueError(\"Required columns 'Content_1' or 'Genre' are missing in the dataframe.\")\n",
    "\n",
    "# Drop rows with missing 'Content_1' or 'Genre'\n",
    "df = df.dropna(subset=['Content_1', 'Genre'])\n",
    "\n",
    "# Reset index to ensure proper indexing\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows after cleaning\n",
    "print(\"Cleaned DataFrame Head:\\n\", df.head())\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Genre_encoded'] = label_encoder.fit_transform(df['Genre'])\n",
    "\n",
    "# Display encoded labels\n",
    "print(\"Encoded Labels:\\n\", df['Genre_encoded'].head())\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_df, val_df, test_df = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "# Check the splits\n",
    "print(f\"Training Set Size: {len(train_df)}\")\n",
    "print(f\"Validation Set Size: {len(val_df)}\")\n",
    "print(f\"Test Set Size: {len(test_df)}\")\n",
    "\n",
    "# Initialize ParsBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n",
    "\n",
    "# Custom dataset class for Persian summaries\n",
    "class PersianMovieGenreDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts.reset_index(drop=True)  # Ensure proper indexing\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PersianMovieGenreDataset(train_df['Content_1'], train_df['Genre_encoded'], tokenizer)\n",
    "val_dataset = PersianMovieGenreDataset(val_df['Content_1'], val_df['Genre_encoded'], tokenizer)\n",
    "test_dataset = PersianMovieGenreDataset(test_df['Content_1'], test_df['Genre_encoded'], tokenizer)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['Genre_encoded']), y=train_df['Genre_encoded'])\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Convert class weights to tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Load the ParsBERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('HooshvareLab/bert-fa-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Define custom training arguments with class weights\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Ensure both evaluation and save strategy are the same\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1'\n",
    ")\n",
    "\n",
    "# Custom Trainer class to handle class weights\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None  # Add your custom metrics computation here\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./parsbert_movie_genre_classifier')\n",
    "tokenizer.save_pretrained('./parsbert_movie_genre_classifier')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
